{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_key = 'GRNd9WWDveclHyNYK8AK3C9pZ10dtQ'\n",
    "client_id = 'WN8X-zWVr39d4wEfVlTU-w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = requests.auth.HTTPBasicAuth(client_id,secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_data = {\n",
    "    'grant_type' : 'password',\n",
    "    'username' : 'removed username',\n",
    "    'password' : 'removed password'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'MyAPI/0.0.1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'MyAPI/0.0.1',\n",
       " 'Authorization': 'bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJ1c2VyIiwiZXhwIjoxNjg2NjgxMzA3LjE0NTc2NCwiaWF0IjoxNjg2NTk0OTA3LjE0NTc2MiwianRpIjoiTmpjTmkwN2tscEEzeDk0cUV1VFhkeWgxYWVaVy1BIiwiY2lkIjoiV044WC16V1ZyMzlkNHdFZlZsVFUtdyIsImxpZCI6InQyXzMzODFsNXNyIiwiYWlkIjoidDJfMzM4MWw1c3IiLCJsY2EiOjE1NDg1Mzc4MDA5MDcsInNjcCI6ImVKeUtWdEpTaWdVRUFBRF9fd056QVNjIiwiZmxvIjo5fQ.mp_gcV5TNZwUbjmIJHSCKN_Fi3Qmfro07ElY6wmIocrrhSGTo_RAWeGPOUoxLvwWH-5ey5v9A4OQ84_PkOEsPu4oIXb9cWEuuaS2xJ21wG0iPHXLj2ipQ6a-Fp3ezfCI6dlDbHYUyh-ZvkakeEuSfjXFkBaeb28hBo7JQHOUK7YbU1UgYWD7dtzDsInbbtGBffXEmgljOVPT0etREHXaezm-p0CnFddGdd9eIPJDOQENrwRFUWh2nyOincvrswKGKuXaQUiqUO-hDuaEE5BfK_3SdXyBDlIV6mctg6oJDV_qnZ0sgQ2N812pKDfYB-eF2HUKTwT7mbe5zD7uN5VBOw'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.post('https://www.reddit.com/api/v1/access_token',auth=auth,data=login_data, headers=headers)\n",
    "response.json()\n",
    "token = response.json()['access_token']\n",
    "headers = {**headers, **{'Authorization': f'bearer {token}'}}\n",
    "headers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SuicideWatch Subreddit - 50,000 Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                post_date kind     subreddit   \n",
      "0     2020-04-23 17:30:39   t3  SuicideWatch  \\\n",
      "1     2019-11-06 07:11:39   t3  SuicideWatch   \n",
      "2     2021-01-16 10:58:58   t3  SuicideWatch   \n",
      "3     2021-01-26 00:42:31   t3  SuicideWatch   \n",
      "4     2019-05-06 18:30:06   t3  SuicideWatch   \n",
      "...                   ...  ...           ...   \n",
      "49995 2019-07-23 16:24:35   t3  SuicideWatch   \n",
      "49996 2020-02-04 03:27:11   t3  SuicideWatch   \n",
      "49997 2021-07-06 19:08:44   t3  SuicideWatch   \n",
      "49998 2022-03-27 03:17:18   t3  SuicideWatch   \n",
      "49999 2020-07-20 06:32:05   t3  SuicideWatch   \n",
      "\n",
      "                                                   title   \n",
      "0      I’ve seen child porn on the internet. I will n...  \\\n",
      "1                        Just called the suicide hotline   \n",
      "2      Just been notified that the cancer in my dad h...   \n",
      "3                                      Police showed up.   \n",
      "4                          throwing away my blades today   \n",
      "...                                                  ...   \n",
      "49995  People always say; \"just\" get a job, \"just\" fi...   \n",
      "49996  People say life gets better, but that’s a fuck...   \n",
      "49997  I hate how I can turn from being fine to feeli...   \n",
      "49998  I think about killing myself everyday but deep...   \n",
      "49999                      Plans foiled by a f**king cat   \n",
      "\n",
      "                                                selftext  \n",
      "0      I saw it on twitter. Multiple accounts. I cybe...  \n",
      "1      ... and was asked to please 'watch my language...  \n",
      "2      In 2015 I lost my mom in a heartbreaking battl...  \n",
      "3      Well last night I got close to killing myself ...  \n",
      "4      been self harming since 12, i’m 22. \\n\\ni’m 3 ...  \n",
      "...                                                  ...  \n",
      "49995  Every conversation I have had about suicide in...  \n",
      "49996  I’m 25. I’ve been a depressed, anxious, suicid...  \n",
      "49997                                                     \n",
      "49998                                                     \n",
      "49999  So I was going to do it yesterday, had my kit ...  \n",
      "\n",
      "[50000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "total_posts = 50000\n",
    "posts_per_request = 100\n",
    "num_requests = total_posts // posts_per_request\n",
    "\n",
    "# List to store all the posts\n",
    "all_posts = []\n",
    "\n",
    "for i in range(num_requests + 1):\n",
    "    # Calculate the offset for each request\n",
    "    offset = i * posts_per_request\n",
    "\n",
    "    # Fetching random posts from the suicide_watch subreddit\n",
    "    res_random = requests.get('https://oauth.reddit.com/r/SuicideWatch/top', headers=headers, params={'limit': posts_per_request, 't': 'all', 'count': offset})\n",
    "\n",
    "    # Extract the posts from the response and add them to the list\n",
    "    random_posts = res_random.json()['data']['children']\n",
    "    all_posts.extend(random_posts)\n",
    "\n",
    "    time.sleep(1)  # Added sleep command, sleeps for 1 second\n",
    "\n",
    "# Shuffle the list of posts\n",
    "random.shuffle(all_posts)\n",
    "\n",
    "# Randomly select the desired number of posts if available, otherwise use all available posts\n",
    "selected_posts = random.sample(all_posts, total_posts) if len(all_posts) >= total_posts else all_posts\n",
    "\n",
    "# Extract the required fields from the posts\n",
    "formatted_posts = []\n",
    "for post in selected_posts:\n",
    "    data = post['data']\n",
    "    formatted_post = {\n",
    "        'post_date': pd.to_datetime(data['created_utc'], unit='s'),\n",
    "        'kind': post['kind'],\n",
    "        'subreddit': data['subreddit'],\n",
    "        'title': data['title'],\n",
    "        'selftext': data['selftext']\n",
    "    }\n",
    "    formatted_posts.append(formatted_post)\n",
    "\n",
    "# Create a DataFrame from the posts\n",
    "suicidewatch_df = pd.DataFrame(formatted_posts)\n",
    "\n",
    "# Print the DataFrame with the post date and other fields\n",
    "print(suicidewatch_df[['post_date', 'kind', 'subreddit', 'title', 'selftext']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_date\n",
      "2018     1998\n",
      "2019     7998\n",
      "2020    22005\n",
      "2021    13997\n",
      "2022     3002\n",
      "2023     1000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count posts for each year\n",
    "year_counts = suicidewatch_df['post_date'].dt.year.value_counts().sort_index()\n",
    "print(year_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "suicidewatch_df.to_csv('SuicideWatch.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depression Subreddit - 50,000 Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                post_date kind   subreddit   \n",
      "0     2018-07-05 15:34:23   t3  depression  \\\n",
      "1     2019-08-28 13:46:06   t3  depression   \n",
      "2     2017-12-06 16:09:05   t3  depression   \n",
      "3     2021-02-18 17:15:50   t3  depression   \n",
      "4     2019-09-26 05:06:33   t3  depression   \n",
      "...                   ...  ...         ...   \n",
      "49995 2019-03-31 17:13:32   t3  depression   \n",
      "49996 2017-09-11 03:44:28   t3  depression   \n",
      "49997 2020-10-24 16:46:02   t3  depression   \n",
      "49998 2021-03-04 12:22:56   t3  depression   \n",
      "49999 2020-06-19 06:17:46   t3  depression   \n",
      "\n",
      "                                                   title   \n",
      "0      When I don’t have a job, I want a job. When I ...  \\\n",
      "1      It’s not that I want to kill myself. But if I ...   \n",
      "2      Imagine if the cure to a broken leg was to run...   \n",
      "3      I think that the worst part of depression is t...   \n",
      "4      When you realize that you have an incredibly e...   \n",
      "...                                                  ...   \n",
      "49995  Becoming a regular at a local coffee shop chan...   \n",
      "49996  Does anybody else ever stay up late so they ca...   \n",
      "49997  I’ve wasted so much time being depressed, that...   \n",
      "49998  Anger as a symptom of depression need to be re...   \n",
      "49999  I don't wanna kill myself, I just don't wanna ...   \n",
      "\n",
      "                                                selftext  \n",
      "0      Do you guys see what I’m saying here. I’m alwa...  \n",
      "1      I don’t think i could ever kill myself but tha...  \n",
      "2      I'm discouraged today! :D \\n\\nEdit: This is a ...  \n",
      "3      The point I realised that I think I collapsed ...  \n",
      "4      My life is objectively very easy but my depres...  \n",
      "...                                                  ...  \n",
      "49995  I was depressed, failing school, etc and I dec...  \n",
      "49996                                                     \n",
      "49997  I can’t imagine how many days I have wasted to...  \n",
      "49998  Depression isn’t just being depressed all the ...  \n",
      "49999  Idk if that makes sense. But I don't want to k...  \n",
      "\n",
      "[50000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "total_posts = 50000\n",
    "posts_per_request = 100\n",
    "num_requests = total_posts // posts_per_request\n",
    "\n",
    "# List to store all the posts\n",
    "all_posts = []\n",
    "\n",
    "for i in range(num_requests + 1):\n",
    "    # Calculate the offset for each request\n",
    "    offset = i * posts_per_request\n",
    "\n",
    "    # Fetching random posts from the depression subreddit\n",
    "    res_random = requests.get('https://oauth.reddit.com/r/depression/top', headers=headers, params={'limit': posts_per_request, 't': 'all', 'count': offset})\n",
    "\n",
    "    # Extract the posts from the response and add them to the list\n",
    "    random_posts = res_random.json()['data']['children']\n",
    "    all_posts.extend(random_posts)\n",
    "\n",
    "    time.sleep(1)  # Added sleep command, sleeps for 1 second\n",
    "\n",
    "# Shuffle the list of posts\n",
    "random.shuffle(all_posts)\n",
    "\n",
    "# Randomly select the desired number of posts if available, otherwise use all available posts\n",
    "selected_posts = random.sample(all_posts, total_posts) if len(all_posts) >= total_posts else all_posts\n",
    "\n",
    "# Extract the required fields from the posts\n",
    "formatted_posts = []\n",
    "for post in selected_posts:\n",
    "    data = post['data']\n",
    "    formatted_post = {\n",
    "        'post_date': pd.to_datetime(data['created_utc'], unit='s'),\n",
    "        'kind': post['kind'],\n",
    "        'subreddit': data['subreddit'],\n",
    "        'title': data['title'],\n",
    "        'selftext': data['selftext']\n",
    "    }\n",
    "    formatted_posts.append(formatted_post)\n",
    "\n",
    "# Create a DataFrame from the posts\n",
    "depression_df = pd.DataFrame(formatted_posts)\n",
    "\n",
    "# Print the DataFrame with the post date and other fields\n",
    "print(depression_df[['post_date', 'kind', 'subreddit', 'title', 'selftext']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_date\n",
      "2017     1502\n",
      "2018     2000\n",
      "2019    18995\n",
      "2020    20501\n",
      "2021     6501\n",
      "2022      501\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count posts for each year\n",
    "year_counts = depression_df['post_date'].dt.year.value_counts().sort_index()\n",
    "print(year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_df.to_csv('depression.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bipolar Subreddit - 50,000 Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                post_date kind subreddit   \n",
      "0     2020-04-16 19:58:43   t3   bipolar  \\\n",
      "1     2020-11-13 17:24:18   t3   bipolar   \n",
      "2     2023-04-07 07:38:22   t3   bipolar   \n",
      "3     2019-04-06 22:10:00   t3   bipolar   \n",
      "4     2018-12-25 01:05:08   t3   bipolar   \n",
      "...                   ...  ...       ...   \n",
      "49995 2019-11-15 14:19:54   t3   bipolar   \n",
      "49996 2020-11-05 19:26:26   t3   bipolar   \n",
      "49997 2023-04-07 07:38:22   t3   bipolar   \n",
      "49998 2019-09-10 19:19:27   t3   bipolar   \n",
      "49999 2016-12-27 18:07:13   t3   bipolar   \n",
      "\n",
      "                                                   title selftext  \n",
      "0      Actually managed to finish one of many quarant...           \n",
      "1                                  Please take your meds           \n",
      "2      Vision when depressed vs. vision when manic: W...           \n",
      "3                   This is exactly how I feel right now           \n",
      "4                                       You’ve got this.           \n",
      "...                                                  ...      ...  \n",
      "49995                                   Light it up baby           \n",
      "49996  I fell into a depression because I lost my gra...           \n",
      "49997  Vision when depressed vs. vision when manic: W...           \n",
      "49998  Me seeing all my friends on FB posting about s...           \n",
      "49999  Carrie Fisher, You will be missed. Thank you f...           \n",
      "\n",
      "[50000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "total_posts = 50000\n",
    "posts_per_request = 100\n",
    "num_requests = total_posts // posts_per_request\n",
    "\n",
    "# List to store all the posts\n",
    "all_posts = []\n",
    "\n",
    "for i in range(num_requests + 1):\n",
    "    # Calculate the offset for each request\n",
    "    offset = i * posts_per_request\n",
    "\n",
    "    # Fetching random posts from the bipolar subreddit\n",
    "    res_random = requests.get('https://oauth.reddit.com/r/bipolar/top', headers=headers, params={'limit': posts_per_request, 't': 'all', 'count': offset})\n",
    "\n",
    "    # Extract the posts from the response and add them to the list\n",
    "    random_posts = res_random.json()['data']['children']\n",
    "    all_posts.extend(random_posts)\n",
    "\n",
    "    time.sleep(1)  # Added sleep command, sleeps for 1 second\n",
    "\n",
    "# Shuffle the list of posts\n",
    "random.shuffle(all_posts)\n",
    "\n",
    "# Randomly select the desired number of posts if available, otherwise use all available posts\n",
    "selected_posts = random.sample(all_posts, total_posts) if len(all_posts) >= total_posts else all_posts\n",
    "\n",
    "# Extract the required fields from the posts\n",
    "formatted_posts = []\n",
    "for post in selected_posts:\n",
    "    data = post['data']\n",
    "    formatted_post = {\n",
    "        'post_date': pd.to_datetime(data['created_utc'], unit='s'),\n",
    "        'kind': post['kind'],\n",
    "        'subreddit': data['subreddit'],\n",
    "        'title': data['title'],\n",
    "        'selftext': data['selftext']\n",
    "    }\n",
    "    formatted_posts.append(formatted_post)\n",
    "\n",
    "# Create a DataFrame from the posts\n",
    "bipolar_df = pd.DataFrame(formatted_posts)\n",
    "\n",
    "# Print the DataFrame with the post date and other fields\n",
    "print(bipolar_df[['post_date', 'kind', 'subreddit', 'title', 'selftext']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_date\n",
      "2016      501\n",
      "2017      499\n",
      "2018     8502\n",
      "2019    20997\n",
      "2020    10998\n",
      "2021     3497\n",
      "2022     3006\n",
      "2023     2000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count posts for each year\n",
    "year_counts = bipolar_df['post_date'].dt.year.value_counts().sort_index()\n",
    "print(year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bipolar_df.to_csv('bipolar.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anxiety Subreddit - 50,000 Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                post_date kind subreddit   \n",
      "0     2019-03-12 22:51:15   t3   Anxiety  \\\n",
      "1     2020-12-30 00:43:10   t3   Anxiety   \n",
      "2     2020-07-30 20:47:32   t3   Anxiety   \n",
      "3     2020-09-15 23:03:58   t3   Anxiety   \n",
      "4     2020-06-24 05:03:00   t3   Anxiety   \n",
      "...                   ...  ...       ...   \n",
      "49995 2018-11-20 08:22:55   t3   Anxiety   \n",
      "49996 2019-03-29 22:58:19   t3   Anxiety   \n",
      "49997 2020-05-20 17:17:45   t3   Anxiety   \n",
      "49998 2017-01-26 19:30:27   t3   Anxiety   \n",
      "49999 2019-02-23 15:29:51   t3   Anxiety   \n",
      "\n",
      "                                                   title   \n",
      "0      Today is my cake day and I wanted to post a pi...  \\\n",
      "1      You ! Reading this ! Reminder to untense your ...   \n",
      "2      I don’t think most people understand how exhau...   \n",
      "3      I drove for 20 minutes on the highway by mysel...   \n",
      "4                 Anxiety makes you look like an asshole   \n",
      "...                                                  ...   \n",
      "49995                                         I am busy.   \n",
      "49996  This is the first time I have been to the movi...   \n",
      "49997  Anxiety Pro Tip: Anxiety thrives on your avoid...   \n",
      "49998  I painted health anxiety... At least how it fe...   \n",
      "49999  Do what you can with what you have wherever yo...   \n",
      "\n",
      "                                                selftext  \n",
      "0                                                         \n",
      "1                                                         \n",
      "2      There’s a lot of physical symptoms such as sha...  \n",
      "3      For the first time ever. I have driving anxiet...  \n",
      "4      Slow responses to message, or no responses at ...  \n",
      "...                                                  ...  \n",
      "49995                                                     \n",
      "49996                                                     \n",
      "49997  Avoiding the anxiety-provoking task feels good...  \n",
      "49998                                                     \n",
      "49999                                                     \n",
      "\n",
      "[50000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "total_posts = 50000\n",
    "posts_per_request = 100\n",
    "num_requests = total_posts // posts_per_request\n",
    "\n",
    "# List to store all the posts\n",
    "all_posts = []\n",
    "\n",
    "for i in range(num_requests + 1):\n",
    "    # Calculate the offset for each request\n",
    "    offset = i * posts_per_request\n",
    "\n",
    "    # Fetching random posts from the anxiety subreddit\n",
    "    res_random = requests.get('https://oauth.reddit.com/r/anxiety/top', headers=headers, params={'limit': posts_per_request, 't': 'all', 'count': offset})\n",
    "\n",
    "    # Extract the posts from the response and add them to the list\n",
    "    random_posts = res_random.json()['data']['children']\n",
    "    all_posts.extend(random_posts)\n",
    "\n",
    "    time.sleep(1)  # Added sleep command, sleeps for 1 second\n",
    "\n",
    "# Shuffle the list of posts\n",
    "random.shuffle(all_posts)\n",
    "\n",
    "# Randomly select the desired number of posts if available, otherwise use all available posts\n",
    "selected_posts = random.sample(all_posts, total_posts) if len(all_posts) >= total_posts else all_posts\n",
    "\n",
    "# Extract the required fields from the posts\n",
    "formatted_posts = []\n",
    "for post in selected_posts:\n",
    "    data = post['data']\n",
    "    formatted_post = {\n",
    "        'post_date': pd.to_datetime(data['created_utc'], unit='s'),\n",
    "        'kind': post['kind'],\n",
    "        'subreddit': data['subreddit'],\n",
    "        'title': data['title'],\n",
    "        'selftext': data['selftext']\n",
    "    }\n",
    "    formatted_posts.append(formatted_post)\n",
    "\n",
    "# Create a DataFrame from the posts\n",
    "anxiety_df = pd.DataFrame(formatted_posts)\n",
    "\n",
    "# Print the DataFrame with the post date and other fields\n",
    "print(anxiety_df[['post_date', 'kind', 'subreddit', 'title', 'selftext']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_date\n",
      "2013      500\n",
      "2017     3001\n",
      "2018     9501\n",
      "2019    14997\n",
      "2020    15997\n",
      "2021     4003\n",
      "2022     2001\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count posts for each year\n",
    "year_counts = anxiety_df['post_date'].dt.year.value_counts().sort_index()\n",
    "print(year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "anxiety_df.to_csv('anxiety.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schizophrenia Subreddit - 50,000 Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                post_date kind      subreddit   \n",
      "0     2021-03-10 23:48:18   t3  schizophrenia  \\\n",
      "1     2021-02-17 05:56:12   t3  schizophrenia   \n",
      "2     2019-03-24 17:48:45   t3  schizophrenia   \n",
      "3     2019-03-24 17:48:45   t3  schizophrenia   \n",
      "4     2020-11-26 17:36:28   t3  schizophrenia   \n",
      "...                   ...  ...            ...   \n",
      "49995 2021-03-13 11:30:16   t3  schizophrenia   \n",
      "49996 2020-12-16 15:07:07   t3  schizophrenia   \n",
      "49997 2020-09-16 14:45:15   t3  schizophrenia   \n",
      "49998 2022-02-02 19:11:15   t3  schizophrenia   \n",
      "49999 2020-12-14 22:37:38   t3  schizophrenia   \n",
      "\n",
      "                                                   title selftext  \n",
      "0                                                    yea           \n",
      "1      Update: I quit my new job. They constantly ber...           \n",
      "2                          It's true and i should say it           \n",
      "3                          It's true and i should say it           \n",
      "4                                               Good job           \n",
      "...                                                  ...      ...  \n",
      "49995  I decided to make slice of life comics includi...           \n",
      "49996                     Only way i can express this...           \n",
      "49997  SELFIE. My dad is schizo and disappeared when ...           \n",
      "49998  Diagnosed SZA in january, this is what I drew ...           \n",
      "49999  my bf is a schizophrenic and sometimes he make...           \n",
      "\n",
      "[50000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "total_posts = 50000\n",
    "posts_per_request = 100\n",
    "num_requests = total_posts // posts_per_request\n",
    "\n",
    "# List to store all the posts\n",
    "all_posts = []\n",
    "\n",
    "for i in range(num_requests + 1):\n",
    "    # Calculate the offset for each request\n",
    "    offset = i * posts_per_request\n",
    "\n",
    "    # Fetching random posts from the schizophrenia subreddit\n",
    "    res_random = requests.get('https://oauth.reddit.com/r/schizophrenia/top', headers=headers, params={'limit': posts_per_request, 't': 'all', 'count': offset})\n",
    "\n",
    "    # Extract the posts from the response and add them to the list\n",
    "    random_posts = res_random.json()['data']['children']\n",
    "    all_posts.extend(random_posts)\n",
    "\n",
    "    time.sleep(1)  # Added sleep command, sleeps for 1 second\n",
    "\n",
    "# Shuffle the list of posts\n",
    "random.shuffle(all_posts)\n",
    "\n",
    "# Randomly select the desired number of posts if available, otherwise use all available posts\n",
    "selected_posts = random.sample(all_posts, total_posts) if len(all_posts) >= total_posts else all_posts\n",
    "\n",
    "# Extract the required fields from the posts\n",
    "formatted_posts = []\n",
    "for post in selected_posts:\n",
    "    data = post['data']\n",
    "    formatted_post = {\n",
    "        'post_date': pd.to_datetime(data['created_utc'], unit='s'),\n",
    "        'kind': post['kind'],\n",
    "        'subreddit': data['subreddit'],\n",
    "        'title': data['title'],\n",
    "        'selftext': data['selftext']\n",
    "    }\n",
    "    formatted_posts.append(formatted_post)\n",
    "\n",
    "# Create a DataFrame from the posts\n",
    "schizophrenia_df = pd.DataFrame(formatted_posts)\n",
    "\n",
    "# Print the DataFrame with the post date and other fields\n",
    "print(schizophrenia_df[['post_date', 'kind', 'subreddit', 'title', 'selftext']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_date\n",
      "2017      501\n",
      "2018     3994\n",
      "2019     9000\n",
      "2020    14998\n",
      "2021     8008\n",
      "2022    11001\n",
      "2023     2498\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count posts for each year\n",
    "year_counts = schizophrenia_df['post_date'].dt.year.value_counts().sort_index()\n",
    "print(year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "schizophrenia_df.to_csv('schizophrenia.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selfharm Subreddit - 50,000 Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                post_date kind subreddit   \n",
      "0     2020-01-05 23:26:46   t3  selfharm  \\\n",
      "1     2020-07-23 17:01:10   t3  selfharm   \n",
      "2     2023-01-11 11:24:11   t3  selfharm   \n",
      "3     2019-06-16 20:20:13   t3  selfharm   \n",
      "4     2020-06-03 19:13:04   t3  selfharm   \n",
      "...                   ...  ...       ...   \n",
      "49995 2020-04-24 10:02:13   t3  selfharm   \n",
      "49996 2020-10-22 11:21:22   t3  selfharm   \n",
      "49997 2022-08-11 22:09:16   t3  selfharm   \n",
      "49998 2020-06-21 22:20:38   t3  selfharm   \n",
      "49999 2020-08-03 15:59:42   t3  selfharm   \n",
      "\n",
      "                                                   title   \n",
      "0                      I hooked up with a guy last night  \\\n",
      "1                           Me, talking to my therapist:   \n",
      "2                             stop glorifying self harm.   \n",
      "3                           \"iTs KiNdA hOt FoR A hoOdiE\"   \n",
      "4                                My cat smelled my blood   \n",
      "...                                                  ...   \n",
      "49995  I feel like people who “only self harm for att...   \n",
      "49996                                             GUYSSS   \n",
      "49997   Selfharm is an addiction, I wont “stop for you🥺”   \n",
      "49998  When you relapse, dont reset your progress. In...   \n",
      "49999                   \"promise me you won't cut again\"   \n",
      "\n",
      "                                                selftext  \n",
      "0      I have bumpy raised scars on my wrist and hund...  \n",
      "1      Her: Okay. If you need to talk before next Fri...  \n",
      "2      it’s not cute. it’s not quirky. to anyone lurk...  \n",
      "3                               like fuck off mom i know  \n",
      "4      I was on the way downstairs to put away my lau...  \n",
      "...                                                  ...  \n",
      "49995  At the end of the day, they are still self har...  \n",
      "49996  IVE REACHED DOUBLE DIGITS\\n10 DAYS AND 4HRS IV...  \n",
      "49997                                                     \n",
      "49998  For instance. If you went 2 months clean and r...  \n",
      "49999  this is honestly one of the worst things you c...  \n",
      "\n",
      "[50000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "total_posts = 50000\n",
    "posts_per_request = 100\n",
    "num_requests = total_posts // posts_per_request\n",
    "\n",
    "# List to store all the posts\n",
    "all_posts = []\n",
    "\n",
    "for i in range(num_requests + 1):\n",
    "    # Calculate the offset for each request\n",
    "    offset = i * posts_per_request\n",
    "\n",
    "    # Fetching random posts from the selfharm subreddit\n",
    "    res_random = requests.get('https://oauth.reddit.com/r/selfharm/top', headers=headers, params={'limit': posts_per_request, 't': 'all', 'count': offset})\n",
    "\n",
    "    # Extract the posts from the response and add them to the list\n",
    "    random_posts = res_random.json()['data']['children']\n",
    "    all_posts.extend(random_posts)\n",
    "\n",
    "    time.sleep(1)  # Added sleep command, sleeps for 1 second\n",
    "\n",
    "# Shuffle the list of posts\n",
    "random.shuffle(all_posts)\n",
    "\n",
    "# Randomly select the desired number of posts if available, otherwise use all available posts\n",
    "selected_posts = random.sample(all_posts, total_posts) if len(all_posts) >= total_posts else all_posts\n",
    "\n",
    "# Extract the required fields from the posts\n",
    "formatted_posts = []\n",
    "for post in selected_posts:\n",
    "    data = post['data']\n",
    "    formatted_post = {\n",
    "        'post_date': pd.to_datetime(data['created_utc'], unit='s'),\n",
    "        'kind': post['kind'],\n",
    "        'subreddit': data['subreddit'],\n",
    "        'title': data['title'],\n",
    "        'selftext': data['selftext']\n",
    "    }\n",
    "    formatted_posts.append(formatted_post)\n",
    "\n",
    "# Create a DataFrame from the posts\n",
    "selfharm_df = pd.DataFrame(formatted_posts)\n",
    "\n",
    "# Print the DataFrame with the post date and other fields\n",
    "print(selfharm_df[['post_date', 'kind', 'subreddit', 'title', 'selftext']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_date\n",
      "2019     4999\n",
      "2020    28002\n",
      "2021    11499\n",
      "2022     3499\n",
      "2023     2001\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count posts for each year\n",
    "year_counts = selfharm_df['post_date'].dt.year.value_counts().sort_index()\n",
    "print(year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfharm_df.to_csv('selfharm.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MentalHealth Subreddit - 50,000 Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                post_date kind     subreddit   \n",
      "0     2019-10-06 01:42:23   t3  mentalhealth  \\\n",
      "1     2021-01-13 15:40:23   t3  mentalhealth   \n",
      "2     2020-03-05 19:02:56   t3  mentalhealth   \n",
      "3     2019-04-15 19:13:30   t3  mentalhealth   \n",
      "4     2020-10-19 02:40:35   t3  mentalhealth   \n",
      "...                   ...  ...           ...   \n",
      "49995 2018-05-07 10:06:37   t3  mentalhealth   \n",
      "49996 2020-01-13 13:18:17   t3  mentalhealth   \n",
      "49997 2022-12-05 10:24:18   t3  mentalhealth   \n",
      "49998 2020-06-14 00:23:07   t3  mentalhealth   \n",
      "49999 2021-11-13 03:49:11   t3  mentalhealth   \n",
      "\n",
      "                                                   title   \n",
      "0                                  Live, Laugh, and Love  \\\n",
      "1      My doctor asked “Do you even want to get bette...   \n",
      "2         Celebrating 6 months since I stopped cutting 🥳   \n",
      "3      Not to brag, but instead of laying in bed ALL ...   \n",
      "4      I am looking for someone who helped me years b...   \n",
      "...                                                  ...   \n",
      "49995  3 weeks clean. I've painted every time I had a...   \n",
      "49996   There is an Arabic saying and it goes like this:   \n",
      "49997                                 Cake saved my life   \n",
      "49998  Hey... so... is anyone else too afraid to leav...   \n",
      "49999  Thank you to the woman who rang me out at Targ...   \n",
      "\n",
      "                                                selftext  \n",
      "0                                                         \n",
      "1      I’m fuming. She told me I need to tighten my b...  \n",
      "2                                                    Yay  \n",
      "3                                                         \n",
      "4      I really hope you see this.\\n\\nI'm. Not sure i...  \n",
      "...                                                  ...  \n",
      "49995                                                     \n",
      "49996  “You want to die? Then throw yourself into the...  \n",
      "49997  6 months ago i decided  to go to a famous brid...  \n",
      "49998  I want to be invisible as I feel so uncomforta...  \n",
      "49999  I went to target and picked out birthday cards...  \n",
      "\n",
      "[50000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "total_posts = 50000\n",
    "posts_per_request = 100\n",
    "num_requests = total_posts // posts_per_request\n",
    "\n",
    "# List to store all the posts\n",
    "all_posts = []\n",
    "\n",
    "for i in range(num_requests + 1):\n",
    "    # Calculate the offset for each request\n",
    "    offset = i * posts_per_request\n",
    "\n",
    "    # Fetching random posts from the mentalhealth subreddit\n",
    "    res_random = requests.get('https://oauth.reddit.com/r/mentalhealth/top', headers=headers, params={'limit': posts_per_request, 't': 'all', 'count': offset})\n",
    "\n",
    "    # Extract the posts from the response and add them to the list\n",
    "    random_posts = res_random.json()['data']['children']\n",
    "    all_posts.extend(random_posts)\n",
    "\n",
    "    time.sleep(1)  # Added sleep command, sleeps for 1 second\n",
    "\n",
    "# Shuffle the list of posts\n",
    "random.shuffle(all_posts)\n",
    "\n",
    "# Randomly select the desired number of posts if available, otherwise use all available posts\n",
    "selected_posts = random.sample(all_posts, total_posts) if len(all_posts) >= total_posts else all_posts\n",
    "\n",
    "# Extract the required fields from the posts\n",
    "formatted_posts = []\n",
    "for post in selected_posts:\n",
    "    data = post['data']\n",
    "    formatted_post = {\n",
    "        'post_date': pd.to_datetime(data['created_utc'], unit='s'),\n",
    "        'kind': post['kind'],\n",
    "        'subreddit': data['subreddit'],\n",
    "        'title': data['title'],\n",
    "        'selftext': data['selftext']\n",
    "    }\n",
    "    formatted_posts.append(formatted_post)\n",
    "\n",
    "# Create a DataFrame from the posts\n",
    "mentalhealth_df = pd.DataFrame(formatted_posts)\n",
    "\n",
    "# Print the DataFrame with the post date and other fields\n",
    "print(mentalhealth_df[['post_date', 'kind', 'subreddit', 'title', 'selftext']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_date\n",
      "2017      999\n",
      "2018     3002\n",
      "2019     8999\n",
      "2020    22500\n",
      "2021    10999\n",
      "2022     3501\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count posts for each year\n",
    "year_counts = mentalhealth_df['post_date'].dt.year.value_counts().sort_index()\n",
    "print(year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentalhealth_df.to_csv('mentalhealth.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PTSD Subreddit - 50,000 Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                post_date kind subreddit   \n",
      "0     2020-10-26 17:54:07   t3      ptsd  \\\n",
      "1     2021-07-20 17:01:12   t3      ptsd   \n",
      "2     2020-10-12 23:15:31   t3      ptsd   \n",
      "3     2019-11-06 00:29:00   t3      ptsd   \n",
      "4     2021-09-03 13:05:59   t3      ptsd   \n",
      "...                   ...  ...       ...   \n",
      "49995 2019-07-29 14:04:12   t3      ptsd   \n",
      "49996 2022-02-27 23:40:17   t3      ptsd   \n",
      "49997 2019-07-11 06:29:33   t3      ptsd   \n",
      "49998 2020-11-18 01:48:50   t3      ptsd   \n",
      "49999 2020-05-14 23:08:29   t3      ptsd   \n",
      "\n",
      "                                                   title   \n",
      "0         I wish people understood the way triggers work  \\\n",
      "1      Here is a diagram that my therapist thinks mig...   \n",
      "2      Does anyone else hate their hometown because o...   \n",
      "3      PTSD makes me see myself as a person that thin...   \n",
      "4      I hate when I go to the emergency room for my ...   \n",
      "...                                                  ...   \n",
      "49995  A cow down the road from me is helping me work...   \n",
      "49996  I feel like no one talks about how you never w...   \n",
      "49997  Two years ago I gave up - today I found out I ...   \n",
      "49998  Annoying trend: Casual use of the term ptsd/tr...   \n",
      "49999  Shoutout to all of the people with PTSD that h...   \n",
      "\n",
      "                                                selftext  \n",
      "0      Although lots of my triggers are very reasonab...  \n",
      "1      I’ve been incredibly suicidal lately, and have...  \n",
      "2      I still live in the city that I grew up and wh...  \n",
      "3      it’s easier to recall the traumas i’ve experie...  \n",
      "4      I don't feel like going into that with you. I ...  \n",
      "...                                                  ...  \n",
      "49995  I recently experienced a violent trauma that l...  \n",
      "49996  Like even on the good days when my ptsd isn’t ...  \n",
      "49997  Two years ago, I made my first of three suicid...  \n",
      "49998  Not to be that overly offended person, but I f...  \n",
      "49999  My memory has been deteriorating a lot over th...  \n",
      "\n",
      "[50000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "total_posts = 50000\n",
    "posts_per_request = 100\n",
    "num_requests = total_posts // posts_per_request\n",
    "\n",
    "# List to store all the posts\n",
    "all_posts = []\n",
    "\n",
    "for i in range(num_requests + 1):\n",
    "    # Calculate the offset for each request\n",
    "    offset = i * posts_per_request\n",
    "\n",
    "    # Fetching random posts from the ptsd subreddit\n",
    "    res_random = requests.get('https://oauth.reddit.com/r/ptsd/top', headers=headers, params={'limit': posts_per_request, 't': 'all', 'count': offset})\n",
    "\n",
    "    # Extract the posts from the response and add them to the list\n",
    "    random_posts = res_random.json()['data']['children']\n",
    "    all_posts.extend(random_posts)\n",
    "\n",
    "    time.sleep(1)  # Added sleep command, sleeps for 1 second\n",
    "\n",
    "# Shuffle the list of posts\n",
    "random.shuffle(all_posts)\n",
    "\n",
    "# Randomly select the desired number of posts if available, otherwise use all available posts\n",
    "selected_posts = random.sample(all_posts, total_posts) if len(all_posts) >= total_posts else all_posts\n",
    "\n",
    "# Extract the required fields from the posts\n",
    "formatted_posts = []\n",
    "for post in selected_posts:\n",
    "    data = post['data']\n",
    "    formatted_post = {\n",
    "        'post_date': pd.to_datetime(data['created_utc'], unit='s'),\n",
    "        'kind': post['kind'],\n",
    "        'subreddit': data['subreddit'],\n",
    "        'title': data['title'],\n",
    "        'selftext': data['selftext']\n",
    "    }\n",
    "    formatted_posts.append(formatted_post)\n",
    "\n",
    "# Create a DataFrame from the posts\n",
    "ptsd_df = pd.DataFrame(formatted_posts)\n",
    "\n",
    "# Print the DataFrame with the post date and other fields\n",
    "print(ptsd_df[['post_date', 'kind', 'subreddit', 'title', 'selftext']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_date\n",
      "2018     1995\n",
      "2019    11496\n",
      "2020    17013\n",
      "2021    11999\n",
      "2022     6497\n",
      "2023     1000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count posts for each year\n",
    "year_counts = ptsd_df['post_date'].dt.year.value_counts().sort_index()\n",
    "print(year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptsd_df.to_csv('ptsd.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCD Subreddit - 50,000 Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                post_date kind subreddit   \n",
      "0     2020-09-30 14:58:28   t3       OCD  \\\n",
      "1     2021-01-08 21:24:28   t3       OCD   \n",
      "2     2021-01-17 08:58:54   t3       OCD   \n",
      "3     2020-09-11 18:57:44   t3       OCD   \n",
      "4     2020-09-25 16:14:24   t3       OCD   \n",
      "...                   ...  ...       ...   \n",
      "49995 2020-11-11 15:01:19   t3       OCD   \n",
      "49996 2021-03-16 01:46:05   t3       OCD   \n",
      "49997 2020-08-10 00:52:59   t3       OCD   \n",
      "49998 2019-12-16 14:58:43   t3       OCD   \n",
      "49999 2021-02-02 04:50:14   t3       OCD   \n",
      "\n",
      "                                                   title selftext  \n",
      "0       Me talking to my OCD first thing in the morning.           \n",
      "1              Intrusive thoughts the moment you wake up           \n",
      "2      this is how ridiculous our thoughts are, we al...           \n",
      "3      This isn't done. But I walked away before I ru...           \n",
      "4                              It really be like this...           \n",
      "...                                                  ...      ...  \n",
      "49995                       Please leave my head forever           \n",
      "49996                                      its like that           \n",
      "49997                          Made me think of this sub           \n",
      "49998                             They just don’t get it           \n",
      "49999  i hate it when people say “i have to organize ...           \n",
      "\n",
      "[50000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "total_posts = 50000\n",
    "posts_per_request = 100\n",
    "num_requests = total_posts // posts_per_request\n",
    "\n",
    "# List to store all the posts\n",
    "all_posts = []\n",
    "\n",
    "for i in range(num_requests + 1):\n",
    "    # Calculate the offset for each request\n",
    "    offset = i * posts_per_request\n",
    "\n",
    "    # Fetching random posts from the OCD subreddit\n",
    "    res_random = requests.get('https://oauth.reddit.com/r/OCD/top', headers=headers, params={'limit': posts_per_request, 't': 'all', 'count': offset})\n",
    "\n",
    "    # Extract the posts from the response and add them to the list\n",
    "    random_posts = res_random.json()['data']['children']\n",
    "    all_posts.extend(random_posts)\n",
    "\n",
    "    time.sleep(1)  # Added sleep command, sleeps for 1 second\n",
    "\n",
    "# Shuffle the list of posts\n",
    "random.shuffle(all_posts)\n",
    "\n",
    "# Randomly select the desired number of posts if available, otherwise use all available posts\n",
    "selected_posts = random.sample(all_posts, total_posts) if len(all_posts) >= total_posts else all_posts\n",
    "\n",
    "# Extract the required fields from the posts\n",
    "formatted_posts = []\n",
    "for post in selected_posts:\n",
    "    data = post['data']\n",
    "    formatted_post = {\n",
    "        'post_date': pd.to_datetime(data['created_utc'], unit='s'),\n",
    "        'kind': post['kind'],\n",
    "        'subreddit': data['subreddit'],\n",
    "        'title': data['title'],\n",
    "        'selftext': data['selftext']\n",
    "    }\n",
    "    formatted_posts.append(formatted_post)\n",
    "\n",
    "# Create a DataFrame from the posts\n",
    "ocd_df = pd.DataFrame(formatted_posts)\n",
    "\n",
    "# Print the DataFrame with the post date and other fields\n",
    "print(ocd_df[['post_date', 'kind', 'subreddit', 'title', 'selftext']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_date\n",
      "2019     5500\n",
      "2020    30998\n",
      "2021    13003\n",
      "2022      499\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count posts for each year\n",
    "year_counts = ocd_df['post_date'].dt.year.value_counts().sort_index()\n",
    "print(year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocd_df.to_csv('ocd.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPD Subreddit - 50,000 Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                post_date kind subreddit   \n",
      "0     2022-09-08 00:13:34   t3       BPD  \\\n",
      "1     2020-12-29 06:14:21   t3       BPD   \n",
      "2     2023-02-20 00:59:53   t3       BPD   \n",
      "3     2020-09-22 18:29:14   t3       BPD   \n",
      "4     2019-11-29 13:49:29   t3       BPD   \n",
      "...                   ...  ...       ...   \n",
      "49995 2021-03-25 17:42:25   t3       BPD   \n",
      "49996 2022-02-09 21:14:04   t3       BPD   \n",
      "49997 2019-03-15 06:17:57   t3       BPD   \n",
      "49998 2021-04-13 15:47:07   t3       BPD   \n",
      "49999 2022-11-04 06:21:49   t3       BPD   \n",
      "\n",
      "                                                   title   \n",
      "0                        I could spend forever in my bed  \\\n",
      "1                                   Don't send that text   \n",
      "2      Being self aware and mentally ill is fucking f...   \n",
      "3      DAE just feel like they’re just gonna end up k...   \n",
      "4      I got told I don’t meet the criteria for BPD a...   \n",
      "...                                                  ...   \n",
      "49995  People misinterpret the reason borderlines sel...   \n",
      "49996                   The best way I’d describe BPD is   \n",
      "49997  I either want to be asleep, high on anything, ...   \n",
      "49998  DAE experience a profoundly sad feeling like y...   \n",
      "49999  FPs are a subconscious trauma reenactment bond...   \n",
      "\n",
      "                                                selftext  \n",
      "0      In my bed I can’t disappoint or upset anyone I...  \n",
      "1      To whomever needs to hear this (including me):...  \n",
      "2      Bro right now I'm having thoughts of how my be...  \n",
      "3      Life seems so bleak to me right now. My BPD is...  \n",
      "4      Hi everyone! This is my first post ever. \\n\\nI...  \n",
      "...                                                  ...  \n",
      "49995  Im currently studying psychology, and we had o...  \n",
      "49996  You feel like you are a child who was forcibly...  \n",
      "49997  Why is living so boring?? I'm only here to pre...  \n",
      "49998  It’s the worst feeling in my repertoire. I can...  \n",
      "49999  Why is it that wherever I go around finding re...  \n",
      "\n",
      "[50000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "total_posts = 50000\n",
    "posts_per_request = 100\n",
    "num_requests = total_posts // posts_per_request\n",
    "\n",
    "# List to store all the posts\n",
    "all_posts = []\n",
    "\n",
    "for i in range(num_requests + 1):\n",
    "    # Calculate the offset for each request\n",
    "    offset = i * posts_per_request\n",
    "\n",
    "    # Fetching random posts from the OCD subreddit\n",
    "    res_random = requests.get('https://oauth.reddit.com/r/BPD/top', headers=headers, params={'limit': posts_per_request, 't': 'all', 'count': offset})\n",
    "\n",
    "    # Extract the posts from the response and add them to the list\n",
    "    random_posts = res_random.json()['data']['children']\n",
    "    all_posts.extend(random_posts)\n",
    "\n",
    "    time.sleep(1)  # Added sleep command, sleeps for 1 second\n",
    "\n",
    "# Shuffle the list of posts\n",
    "random.shuffle(all_posts)\n",
    "\n",
    "# Randomly select the desired number of posts if available, otherwise use all available posts\n",
    "selected_posts = random.sample(all_posts, total_posts) if len(all_posts) >= total_posts else all_posts\n",
    "\n",
    "# Extract the required fields from the posts\n",
    "formatted_posts = []\n",
    "for post in selected_posts:\n",
    "    data = post['data']\n",
    "    formatted_post = {\n",
    "        'post_date': pd.to_datetime(data['created_utc'], unit='s'),\n",
    "        'kind': post['kind'],\n",
    "        'subreddit': data['subreddit'],\n",
    "        'title': data['title'],\n",
    "        'selftext': data['selftext']\n",
    "    }\n",
    "    formatted_posts.append(formatted_post)\n",
    "\n",
    "# Create a DataFrame from the posts\n",
    "bpd_df = pd.DataFrame(formatted_posts)\n",
    "\n",
    "# Print the DataFrame with the post date and other fields\n",
    "print(bpd_df[['post_date', 'kind', 'subreddit', 'title', 'selftext']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_date\n",
      "2018     2500\n",
      "2019     6000\n",
      "2020    13000\n",
      "2021    16500\n",
      "2022     6998\n",
      "2023     5002\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count posts for each year\n",
    "year_counts = bpd_df['post_date'].dt.year.value_counts().sort_index()\n",
    "print(year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpd_df.to_csv('bpd.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addiction Subreddit - 50,000 Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "total_posts = 50000\n",
    "posts_per_request = 100\n",
    "num_requests = total_posts // posts_per_request\n",
    "\n",
    "# List to store all the posts\n",
    "all_posts = []\n",
    "\n",
    "for i in range(num_requests + 1):\n",
    "    # Calculate the offset for each request\n",
    "    offset = i * posts_per_request\n",
    "\n",
    "    # Fetching random posts from the addiction subreddit\n",
    "    res_random = requests.get('https://oauth.reddit.com/r/addiction/top', headers=headers, params={'limit': posts_per_request, 't': 'all', 'count': offset})\n",
    "\n",
    "    # Extract the posts from the response and add them to the list\n",
    "    random_posts = res_random.json()['data']['children']\n",
    "    all_posts.extend(random_posts)\n",
    "\n",
    "    time.sleep(1)  # Added sleep command, sleeps for 1 second\n",
    "\n",
    "# Shuffle the list of posts\n",
    "random.shuffle(all_posts)\n",
    "\n",
    "# Randomly select the desired number of posts if available, otherwise use all available posts\n",
    "selected_posts = random.sample(all_posts, total_posts) if len(all_posts) >= total_posts else all_posts\n",
    "\n",
    "# Extract the required fields from the posts\n",
    "formatted_posts = []\n",
    "for post in selected_posts:\n",
    "    data = post['data']\n",
    "    formatted_post = {\n",
    "        'post_date': pd.to_datetime(data['created_utc'], unit='s'),\n",
    "        'kind': post['kind'],\n",
    "        'subreddit': data['subreddit'],\n",
    "        'title': data['title'],\n",
    "        'selftext': data['selftext']\n",
    "    }\n",
    "    formatted_posts.append(formatted_post)\n",
    "\n",
    "# Create a DataFrame from the posts\n",
    "addiction_df = pd.DataFrame(formatted_posts)\n",
    "\n",
    "# Print the DataFrame with the post date and other fields\n",
    "print(addiction_df[['post_date', 'kind', 'subreddit', 'title', 'selftext']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_date\n",
      "2016      497\n",
      "2018      999\n",
      "2019    13001\n",
      "2020    17008\n",
      "2021     8497\n",
      "2022     9998\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count posts for each year\n",
    "year_counts = addiction_df['post_date'].dt.year.value_counts().sort_index()\n",
    "print(year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addiction_df.to_csv('addiction.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADHD subreddit - 50,000 posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                post_date kind subreddit   \n",
      "0     2021-05-11 12:20:09   t3      ADHD  \\\n",
      "1     2021-03-09 14:28:11   t3      ADHD   \n",
      "2     2021-07-25 22:19:16   t3      ADHD   \n",
      "3     2022-10-07 17:39:29   t3      ADHD   \n",
      "4     2021-02-26 15:06:56   t3      ADHD   \n",
      "...                   ...  ...       ...   \n",
      "49995 2023-03-23 11:56:04   t3      ADHD   \n",
      "49996 2020-11-24 13:33:17   t3      ADHD   \n",
      "49997 2020-07-09 16:37:16   t3      ADHD   \n",
      "49998 2021-07-05 10:10:27   t3      ADHD   \n",
      "49999 2020-11-01 19:48:38   t3      ADHD   \n",
      "\n",
      "                                                   title   \n",
      "0      Moment of silence for all the time spent procr...  \\\n",
      "1      Motivation Tip: Dopamine rewards don't really ...   \n",
      "2      My dad just told me something that really open...   \n",
      "3      The three great ADHD virtues: loitering, defia...   \n",
      "4      How is everything so BORING yet OVERWHELMING a...   \n",
      "...                                                  ...   \n",
      "49995  devastated to find out that a tidy living envi...   \n",
      "49996  A teacher once told me, “ADHD isn’t when you d...   \n",
      "49997  You can always tell whether ADHD \"tips\" were w...   \n",
      "49998                  Holy shit, ADHD is fucking awful.   \n",
      "49999  I CLEANED MY ENTIRE APPARTMENT ALL BY MYSELF!!...   \n",
      "\n",
      "                                                selftext  \n",
      "0      When you’re stuck on your phone. You don’t all...  \n",
      "1      I've tried nearly every mainstream focus tip t...  \n",
      "2      He told me to “just get out of your head and d...  \n",
      "3      I've been on this sub for a couple years and t...  \n",
      "4      My room is a mess yet every time I tidy it it ...  \n",
      "...                                                  ...  \n",
      "49995  undiagnosed ADHD till i was 24, always told pe...  \n",
      "49996  I’ve always loved looking at it that way. Sure...  \n",
      "49997  PhD expert who doesnt actually have ADHD: \"Mak...  \n",
      "49998  5 minutes. that's how long it took to get a ta...  \n",
      "49999  Sorry for yelling guys, but I'm so happy! Omg ...  \n",
      "\n",
      "[50000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "total_posts = 50000\n",
    "posts_per_request = 100\n",
    "num_requests = total_posts // posts_per_request\n",
    "\n",
    "# List to store all the posts\n",
    "all_posts = []\n",
    "\n",
    "for i in range(num_requests + 1):\n",
    "    # Calculate the offset for each request\n",
    "    offset = i * posts_per_request\n",
    "\n",
    "    # Fetching random posts from the OCD subreddit\n",
    "    res_random = requests.get('https://oauth.reddit.com/r/ADHD/top', headers=headers, params={'limit': posts_per_request, 't': 'all', 'count': offset})\n",
    "\n",
    "    # Extract the posts from the response and add them to the list\n",
    "    random_posts = res_random.json()['data']['children']\n",
    "    all_posts.extend(random_posts)\n",
    "\n",
    "    time.sleep(1)  # Added sleep command, sleeps for 1 second\n",
    "\n",
    "# Shuffle the list of posts\n",
    "random.shuffle(all_posts)\n",
    "\n",
    "# Randomly select the desired number of posts if available, otherwise use all available posts\n",
    "selected_posts = random.sample(all_posts, total_posts) if len(all_posts) >= total_posts else all_posts\n",
    "\n",
    "# Extract the required fields from the posts\n",
    "formatted_posts = []\n",
    "for post in selected_posts:\n",
    "    data = post['data']\n",
    "    formatted_post = {\n",
    "        'post_date': pd.to_datetime(data['created_utc'], unit='s'),\n",
    "        'kind': post['kind'],\n",
    "        'subreddit': data['subreddit'],\n",
    "        'title': data['title'],\n",
    "        'selftext': data['selftext']\n",
    "    }\n",
    "    formatted_posts.append(formatted_post)\n",
    "\n",
    "# Create a DataFrame from the posts\n",
    "adhd_df = pd.DataFrame(formatted_posts)\n",
    "\n",
    "# Print the DataFrame with the post date and other fields\n",
    "print(adhd_df[['post_date', 'kind', 'subreddit', 'title', 'selftext']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_date\n",
      "2019      501\n",
      "2020    14997\n",
      "2021    19498\n",
      "2022    11004\n",
      "2023     4000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count posts for each year\n",
    "year_counts = adhd_df['post_date'].dt.year.value_counts().sort_index()\n",
    "print(year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhd_df.to_csv('adhd.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My CSV files are addiction.csv, adhd.csv, anxiety.csv, bipolar.csv, bpd.csv, depression.csv, mentalhealth.csv, ocd.csv, ptsd.csv, schizophrenia.csv, selfharm.csv, and SuicideWatch.csv - the column rows for every CSV are the same... they are: post_date,kind,subreddit,title,selftext"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
