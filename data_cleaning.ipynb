{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Annotated DFs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_post_df = pd.read_csv(\"/Users/brittanyharding/LHL-Projects/SM-suicide_detection/annotated_train_df/annotated_post_df.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Slang Dictionary to Best Understand MH posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_dict = {\n",
    "    \"mh\": \"mental health\",\n",
    "    \"adhd\": \"attention deficit hyperactivity disorder\",\n",
    "    \"asd\": \"autism spectrum disorder\",\n",
    "    \"bpd\": \"borderline personality disorder\",\n",
    "    \"ocd\": \"obsessive-compulsive disorder\",\n",
    "    \"ptsd\": \"post-traumatic stress disorder\",\n",
    "    \"gad\": \"generalized anxiety disorder\",\n",
    "    \"mdd\": \"major depressive disorder\",\n",
    "    \"sad\": \"social anxiety disorder or seasonal affective disorder\",\n",
    "    \"bd\": \"bipolar disorder\",\n",
    "    \"dx\": \"diagnosis\",\n",
    "    \"tx\": \"treatment\",\n",
    "    \"rx\": \"prescription\",\n",
    "    \"cbt\": \"cognitive behavioral therapy\",\n",
    "    \"dbt\": \"dialectical behavior therapy\",\n",
    "    \"emdr\": \"eye movement desensitization and reprocessing\",\n",
    "    \"act\": \"acceptance and commitment therapy\",\n",
    "    \"ssri\": \"selective serotonin reuptake inhibitor\",\n",
    "    \"snri\": \"serotonin and norepinephrine reuptake inhibitor\",\n",
    "    \"nd\": \"neurodivergent\",\n",
    "    \"nt\": \"neurotypical\",\n",
    "    \"dsm\": \"diagnostic and statistical manual of mental disorders\",\n",
    "    \"stims\": \"self-stimulatory behavior\",\n",
    "    \"stimming\": \"self-stimulatory behavior\",\n",
    "    \"tw\": \"trigger warning\",\n",
    "    \"bp\": \"bipolar\",\n",
    "    \"apd\": \"antisocial personality disorder\",\n",
    "    \"avpd\": \"avoidant personality disorder\",\n",
    "    \"did\": \"dissociative identity disorder\",\n",
    "    \"spd\": \"schizoid personality disorder or sensory processing disorder\",\n",
    "    \"ed\": \"eating disorder\",\n",
    "    \"an\": \"anorexia nervosa\",\n",
    "    \"bn\": \"bulimia nervosa\",\n",
    "    \"bed\": \"binge eating disorder\",\n",
    "    \"osfed\": \"other specified feeding or eating disorder\",\n",
    "    \"sh\": \"self harm\",\n",
    "    \"si\": \"suicidal ideation\",\n",
    "    \"iop\": \"intensive outpatient\",\n",
    "    \"ip\": \"inpatient\",\n",
    "    \"tms\": \"transcranial magnetic stimulation\",\n",
    "    \"ect\": \"electroconvulsive therapy\",\n",
    "    \"np\": \"nurse practitioner\",\n",
    "    \"pa\": \"physician's assistant\",\n",
    "    \"psyd\": \"doctor of psychology\",\n",
    "    \"md\": \"medical doctor\",\n",
    "    \"lcsw\": \"licensed clinical social worker\",\n",
    "    \"lmft\": \"licensed marriage and family therapist\",\n",
    "    \"lpcc\": \"licensed professional clinical counselor\",\n",
    "    \"erp\": \"exposure and response prevention\",\n",
    "    \"nos\": \"not otherwise specified\",\n",
    "    \"pe\": \"prolonged exposure\",\n",
    "    \"sa\": \"social anxiety\",\n",
    "    \"meds\": \"medications\",\n",
    "    \"dpdr\": \"depersonalization-derealization\",\n",
    "    \"pdoc\": \"psychiatrist\",\n",
    "    \"tdoc\": \"therapist\",\n",
    "    \"brain fog\": \"difficulty thinking clearly\",\n",
    "    \"grounding\": \"techniques used to help stay in the present moment\",\n",
    "    \"disso\": \"dissociation\",\n",
    "    \"mania\": \"manic episode\",\n",
    "    \"hypo\": \"hypomania\",\n",
    "    \"zaps\": \"brain zaps\",\n",
    "    \"med change\": \"changing medications\",\n",
    "    \"titrate\": \"gradually increase or decrease medication dose\",\n",
    "    \"psych ward\": \"psychiatric ward\",\n",
    "    \"5150\": \"involuntary psychiatric hold\",\n",
    "    \"in crisis\": \"experiencing a mental health emergency\",\n",
    "    \"meltdown\": \"emotional breakdown\",\n",
    "    \"shutdown\": \"a response to stress or overwhelm often experienced by people with autism\",\n",
    "    \"spiraling\": \"quickly worsening mental health symptoms\",\n",
    "    \"flashbacks\": \"intrusive memories of a traumatic event\",\n",
    "    \"grounding techniques\": \"methods used to bring oneself back into the present moment\",\n",
    "    \"gaslighting\": \"manipulative behavior to make someone doubt their own experiences\",\n",
    "    \"ghosting\": \"ending a relationship by suddenly and without explanation withdrawing from all communication\",\n",
    "    \"breadcrumbing\": \"the act of sending out flirtatious, but non-committal messages in order to lure a romantic partner\",\n",
    "    \"shed\": \"self-harmed\",\n",
    "    \"op\": \"original poster\",\n",
    "    \"tl;dr\": \"too long; didn't read\",\n",
    "    \"dae\": \"does anyone else\",\n",
    "    \"iirc\": \"if i remember correctly\",\n",
    "    \"ftfy\": \"fixed that for you\",\n",
    "    \"ama\": \"ask me anything\",\n",
    "    \"amaa\": \"ask me almost anything\",\n",
    "    \"iama\": \"i am a...\",\n",
    "    \"cmv\": \"change my view\",\n",
    "    \"lpt\": \"life pro tip\",\n",
    "    \"eli5\": \"explain like i'm 5\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"imho\": \"in my honest opinion\",\n",
    "    \"ymmv\": \"your mileage may vary\",\n",
    "    \"so\": \"significant other\",\n",
    "    \"ysk\": \"you should know\",\n",
    "    \"psa\": \"public service announcement\",\n",
    "    \"til\": \"today i learned\",\n",
    "    'nsfl': \"not safe for life\",\n",
    "    'nsfw': \"not safe for work\",\n",
    "    'wip': \"work in progress\",\n",
    "    'ikyky': \"if you know you know\",\n",
    "    'finna': \"getting ready to do something\",\n",
    "    'med': \"medication\",\n",
    "    'meds': \"medications\",\n",
    "    'od d': \"overdosed\",\n",
    "    'od': \"overdose\",\n",
    "    'sm': \"social media\",\n",
    "    'pci': \"post-crisis intervention\",\n",
    "    'hr': \"human resources\",\n",
    "    \"cap\": \"lie\",\n",
    "    \"no cap\": \"no lie\",\n",
    "    \"lit\": \"amazing or exciting\",\n",
    "    \"on fleek\": \"perfect or flawless\",\n",
    "    \"flex\": \"showing off or bragging\",\n",
    "    \"clout\": \"influence or popularity\",\n",
    "    \"savage\": \"fierce or ruthless\",\n",
    "    \"GOAT\": \"greatest of all time\",\n",
    "    \"bae\": \"before anyone else\",\n",
    "    \"chill\": \"calm down\",\n",
    "    \"thirsty\": \"desperate for attention\",\n",
    "    \"woke\": \"aware or knowledgeable\",\n",
    "    \"AF\": \"as fuck (emphasizing something)\",\n",
    "    \"squad\": \"group of friends\",\n",
    "    \"hater\": \"person who dislikes or criticizes others\",\n",
    "    \"flexin'\": \"showing off or boasting\",\n",
    "    \"gig\": \"job or event\",\n",
    "    \"shook\": \"surprised or shocked\",\n",
    "    \"vibe\": \"atmosphere or feeling\",\n",
    "    \"thicc\": \"curvy or voluptuous\",\n",
    "    \"salty\": \"bitter or resentful\",\n",
    "    \"basic\": \"unoriginal or mainstream\",\n",
    "    \"extra\": \"over the top or excessive\",\n",
    "    \"depressy\": \"depressed\",\n",
    "    \"grippy sock vacation\": \"psychiatric care\",\n",
    "    \"doom scrolling\": \"obsessively scrolling\",\n",
    "    \"sewerslide\": \"suicide\",\n",
    "    \"menty b\": \"mental breakdown\",\n",
    "    \"unalive\": \"die\",\n",
    "    \"i had pasta tonight\": \"having suicidal thoughts\",\n",
    "    \"i finished my shampoo and conditioner at the same time\": \"having suicidal thoughts\",\n",
    "    \"plug\": \"drug dealer\",\n",
    "    \"420\": \"marijuana\",\n",
    "    \"burnout\": \"heavy drug user\",\n",
    "    \"clucking\": \"withdrawal\",\n",
    "    \"cold turkey\": \"abrupt withdrawal\",\n",
    "    \"cooker\": \"heavy drug user\",\n",
    "    \"dial-a-doping\": \"drug delivery\",\n",
    "    \"doc\": \"drug of choice\",\n",
    "    \"faded\": \"intoxicated\",\n",
    "    \"üç≠\": \"drug user or drug supplier\",\n",
    "    \"molly\": \"mdma\",\n",
    "    \"pnp\": \"party and play\",\n",
    "    \"snowflake\": \"cocaine or overly sensitive person\",\n",
    "    \"turnt\": \"under influence\",\n",
    "    \"zombie\": \"heavy drug user\",\n",
    "    \"kys\": \"kill yourself\",\n",
    "    \"kms\": \"kill myself\",\n",
    "    \"merked\": \"very drunk or beaten up\",\n",
    "    \"#sue\": \"suicide\",\n",
    "    \"dirl\": \"die in real life\",\n",
    "    \"#ana\": \"anorexia\",\n",
    "    \"#deb\": \"depression\",\n",
    "    \"cat scratches\": \"superficial self-harm cuts\",\n",
    "    \"styros\": \"self-harm cuts to dermis\",\n",
    "    \"beans\": \"self-harm cuts to fat layer\",\n",
    "    \"x\": \"ecstasy\",\n",
    "    \"xan\": \"xanax\",\n",
    "    \"back-to-school necklace\": \"noose reference\",\n",
    "    \"#cuts\": \"self-harm discussion\",\n",
    "    \"#cu46\": \"sexual meet-up\",\n",
    "    \"#ednos\": \"unspecified eating disorder\",\n",
    "    \"#kush\": \"marijuana discussion\",\n",
    "    \"#mias\": \"bulimia discussion\",\n",
    "    \"#secretsociety123\": \"self-harm community\",\n",
    "    \"#selfharmmm\": \"self-harm discussion\",\n",
    "    \"#svv\": \"self-harm discussion\",\n",
    "    \"#tina\": \"crystal meth\",\n",
    "    \"#thinsp\": \"unhealthy weight loss promotion\",\n",
    "    \"#proana\": \"pro-anorexia promotion\",\n",
    "    \"#promia\": \"pro-bulimia promotion\",\n",
    "    \"yeet\": \"self-harm - cutting\",\n",
    "    \"yeets\": \"self-harm wounds/scars (recent)\",\n",
    "    \"yeeting\": \"cutting\",\n",
    "    \"final yeet\": \"suicide attempt\",\n",
    "    \"slicey bois\": \"razors\",\n",
    "    \"barcode\": \"cluster of self-harm wounds/scars\"\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Create a spell checker object\n",
    "spell_checker = SpellChecker()\n",
    "\n",
    "# Add the slang terms to the spell checker's vocabulary\n",
    "spell_checker.word_frequency.load_words(slang_dict.keys())\n",
    "\n",
    "# Function to perform spell checking while preserving slang terms\n",
    "def spell_check(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "\n",
    "    # Iterate over the words and check if they are in the spell checker's vocabulary\n",
    "    for word in words:\n",
    "        # Check if the word is a slang term\n",
    "        if word.lower() in slang_dict:\n",
    "            corrected_word = word\n",
    "        else:\n",
    "            # Check if the word is misspelled\n",
    "            if word not in spell_checker:\n",
    "                # Get the corrected spelling for the word\n",
    "                corrected_word = spell_checker.correction(word)\n",
    "                # Handle the case where the correction is None\n",
    "                if corrected_word is None:\n",
    "                    corrected_word = word\n",
    "            else:\n",
    "                corrected_word = word\n",
    "        \n",
    "        corrected_words.append(corrected_word)\n",
    "\n",
    "    # Join the corrected words back into a string\n",
    "    corrected_text = ' '.join(corrected_words) if corrected_words else ''\n",
    "\n",
    "    return corrected_text\n",
    "\n",
    "# Apply the spell_check function to the 'post' column in your annotated_post_df DataFrame\n",
    "annotated_post_df['post'] = annotated_post_df['post'].apply(spell_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_all(data_frame, column):\n",
    "    # Create new column names for the preprocessed text\n",
    "    new_column = 'preprocessed_' + column\n",
    "    \n",
    "    # Fill null values with empty strings\n",
    "    data_frame[column].fillna('', inplace=True)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    data_frame[new_column] = data_frame[column].apply(lambda x: x.translate(str.maketrans(\"\", \"\", string.punctuation)) if isinstance(x, str) else x)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    data_frame[new_column] = data_frame[new_column].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    data_frame[new_column] = data_frame[new_column].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]) if isinstance(x, str) else x)\n",
    "    \n",
    "    # Stem the tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    data_frame[new_column] = data_frame[new_column].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]) if isinstance(x, str) else x)\n",
    "    \n",
    "    return data_frame\n",
    "\n",
    "# Apply the preprocess_all function to annotated_post_df\n",
    "annotated_post_df = preprocess_all(annotated_post_df, 'post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_date</th>\n",
       "      <th>username</th>\n",
       "      <th>post</th>\n",
       "      <th>classification</th>\n",
       "      <th>preprocessed_post</th>\n",
       "      <th>lowercase_punc_removal_post</th>\n",
       "      <th>tokenized_preprocessed_post</th>\n",
       "      <th>tokenized_lowercase_punc_removal_post</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_148e1wn</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>2023-06-13 12:15</td>\n",
       "      <td>AdEffective9235</td>\n",
       "      <td>Setraline: I was on Setraline for i years Mayb...</td>\n",
       "      <td>Crisis</td>\n",
       "      <td>setralin setralin year mayb closer 10 got poin...</td>\n",
       "      <td>setraline i was on setraline for i years maybe...</td>\n",
       "      <td>[setralin, setralin, year, mayb, closer, 10, g...</td>\n",
       "      <td>[setraline, i, was, on, setraline, for, i, yea...</td>\n",
       "      <td>-0.019798</td>\n",
       "      <td>0.523636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_148dt70</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>2023-06-13 12:02</td>\n",
       "      <td>Nzaqj</td>\n",
       "      <td>help his I don't know if this is the right pla...</td>\n",
       "      <td>Struggling</td>\n",
       "      <td>help dont know right place subject anyway prob...</td>\n",
       "      <td>help his i dont know if this is the right plac...</td>\n",
       "      <td>[help, dont, know, right, place, subject, anyw...</td>\n",
       "      <td>[help, his, i, dont, know, if, this, is, the, ...</td>\n",
       "      <td>-0.002381</td>\n",
       "      <td>0.349048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   identifier subreddit         post_date         username   \n",
       "0  t3_148e1wn   Anxiety  2023-06-13 12:15  AdEffective9235  \\\n",
       "1  t3_148dt70   Anxiety  2023-06-13 12:02            Nzaqj   \n",
       "\n",
       "                                                post classification   \n",
       "0  Setraline: I was on Setraline for i years Mayb...         Crisis  \\\n",
       "1  help his I don't know if this is the right pla...     Struggling   \n",
       "\n",
       "                                   preprocessed_post   \n",
       "0  setralin setralin year mayb closer 10 got poin...  \\\n",
       "1  help dont know right place subject anyway prob...   \n",
       "\n",
       "                         lowercase_punc_removal_post   \n",
       "0  setraline i was on setraline for i years maybe...  \\\n",
       "1  help his i dont know if this is the right plac...   \n",
       "\n",
       "                         tokenized_preprocessed_post   \n",
       "0  [setralin, setralin, year, mayb, closer, 10, g...  \\\n",
       "1  [help, dont, know, right, place, subject, anyw...   \n",
       "\n",
       "               tokenized_lowercase_punc_removal_post  sentiment_polarity   \n",
       "0  [setraline, i, was, on, setraline, for, i, yea...           -0.019798  \\\n",
       "1  [help, his, i, dont, know, if, this, is, the, ...           -0.002381   \n",
       "\n",
       "   sentiment_subjectivity  \n",
       "0                0.523636  \n",
       "1                0.349048  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_post_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def basic_clean(data_frame, column):\n",
    "    # Create new column names for the preprocessed text\n",
    "    new_column = 'lowercase_punc_removal_' + column\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    data_frame[new_column] = data_frame[column].apply(lambda x: x.translate(str.maketrans(\"\", \"\", string.punctuation)).lower() if isinstance(x, str) else x)\n",
    "    \n",
    "    return data_frame\n",
    "\n",
    "# Apply the basic_clean function to annotated_post_df\n",
    "annotated_post_df = basic_clean(annotated_post_df, 'post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_text(data_frame, column):\n",
    "    # Create new column name for the tokenized text\n",
    "    tokenized_column = 'tokenized_' + column\n",
    "    \n",
    "    # Fill null values with empty strings\n",
    "    data_frame[column].fillna('', inplace=True)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    data_frame[tokenized_column] = data_frame[column].apply(lambda x: word_tokenize(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    return data_frame\n",
    "\n",
    "# Apply the tokenize_text function to annotated_post_df\n",
    "annotated_post_df = tokenize_text(annotated_post_df, 'preprocessed_post')\n",
    "annotated_post_df = tokenize_text(annotated_post_df, 'lowercase_punc_removal_post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "annotated_post_df['sentiment_polarity'] = annotated_post_df['post'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "annotated_post_df['sentiment_subjectivity'] = annotated_post_df['post'].apply(lambda x: TextBlob(x).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save annotated_post_df to clean_annotated_posts.csv\n",
    "annotated_post_df.to_csv('clean_annotated_posts.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Data Preparation\n",
    "clean_annotated_posts = pd.read_csv('clean_annotated_posts.csv')\n",
    "\n",
    "# Split the data into train, val, and test sets\n",
    "# Split the data into train, val, and test sets\n",
    "train_data = clean_annotated_posts.sample(frac=0.8, random_state=42)\n",
    "val_data = clean_annotated_posts.sample(frac=0.1, random_state=42)\n",
    "test_data = clean_annotated_posts.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Convert the text labels to numeric labels\n",
    "label_mapping = {\n",
    "    'crisis': 0,\n",
    "    'struggling': 1,\n",
    "    'recovery/management': 2,\n",
    "    'support/advice': 3\n",
    "}\n",
    "\n",
    "train_data['label'] = train_data['classification'].map(label_mapping)\n",
    "val_data['label'] = val_data['classification'].map(label_mapping)\n",
    "test_data['label'] = test_data['classification'].map(label_mapping)\n",
    "\n",
    "# Step 2: Fine-tuning BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_mapping))\n",
    "\n",
    "# Step 3: Dataset Preparation\n",
    "class PostDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.iloc[index]['tokenized_preprocessed_post']\n",
    "        label = self.data.iloc[index]['label']\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoded_input['input_ids'].squeeze()\n",
    "        attention_mask = encoded_input['attention_mask'].squeeze()\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "max_length = 128\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = PostDataset(train_data, tokenizer, max_length)\n",
    "val_dataset = PostDataset(val_data, tokenizer, max_length)\n",
    "test_dataset = PostDataset(test_data, tokenizer, max_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Step 4: Model Architecture\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Step 5: Training\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = total_loss / total_samples\n",
    "\n",
    "    # Step 6: Evaluation\n",
    "    model.eval()\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            val_predictions.extend(predictions.tolist())\n",
    "            val_labels.extend(labels.tolist())\n",
    "\n",
    "    val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Step 7: Inference\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, attention_mask, _ = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        test_predictions.extend(predictions.tolist())\n",
    "\n",
    "# Create a new DataFrame for inferred labels\n",
    "inferred_labels = pd.DataFrame({'post': test_data['post'], 'inferred_label': test_predictions})\n",
    "inferred_labels.to_csv('inferred_labels.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test BERT on 100 row sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/brittanyharding/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([16])) must be the same as input size (torch.Size([16, 4]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 93\u001b[0m\n\u001b[1;32m     89\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     91\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 93\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[1;32m     94\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m     95\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1600\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mproblem_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulti_label_classification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         loss_fct \u001b[39m=\u001b[39m BCEWithLogitsLoss()\n\u001b[0;32m-> 1600\u001b[0m         loss \u001b[39m=\u001b[39m loss_fct(logits, labels)\n\u001b[1;32m   1601\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1602\u001b[0m     output \u001b[39m=\u001b[39m (logits,) \u001b[39m+\u001b[39m outputs[\u001b[39m2\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:720\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 720\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target,\n\u001b[1;32m    721\u001b[0m                                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    722\u001b[0m                                               pos_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_weight,\n\u001b[1;32m    723\u001b[0m                                               reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/functional.py:3163\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3160\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[0;32m-> 3163\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()))\n\u001b[1;32m   3165\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([16])) must be the same as input size (torch.Size([16, 4]))"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Data Preparation\n",
    "clean_annotated_posts = pd.read_csv('clean_annotated_posts.csv')\n",
    "\n",
    "# Sample a smaller portion of data\n",
    "sample_size = 100  # Adjust the sample size as desired\n",
    "clean_annotated_posts = clean_annotated_posts.sample(sample_size)\n",
    "\n",
    "# Split the data into train, val, and test sets\n",
    "train_data = clean_annotated_posts.sample(frac=0.6, random_state=42)\n",
    "val_data = clean_annotated_posts.sample(frac=0.2, random_state=42)\n",
    "test_data = clean_annotated_posts.sample(frac=0.2, random_state=42)\n",
    "\n",
    "# Convert the text labels to numeric labels\n",
    "label_mapping = {\n",
    "    'crisis': 0,\n",
    "    'struggling': 1,\n",
    "    'recovery/management': 2,\n",
    "    'support/advice': 3\n",
    "}\n",
    "\n",
    "train_data['label'] = train_data['classification'].map(label_mapping)\n",
    "val_data['label'] = val_data['classification'].map(label_mapping)\n",
    "test_data['label'] = test_data['classification'].map(label_mapping)\n",
    "\n",
    "# Step 2: Fine-tuning BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_mapping))\n",
    "\n",
    "\n",
    "# Step 3: Dataset Preparation\n",
    "class PostDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.iloc[index]['tokenized_preprocessed_post']\n",
    "        label = self.data.iloc[index]['label']\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoded_input['input_ids'].squeeze()\n",
    "        attention_mask = encoded_input['attention_mask'].squeeze()\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "max_length = 128\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = PostDataset(train_data, tokenizer, max_length)\n",
    "val_dataset = PostDataset(val_data, tokenizer, max_length)\n",
    "test_dataset = PostDataset(test_data, tokenizer, max_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Step 4: Model Architecture\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Step 5: Training\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = total_loss / total_samples\n",
    "\n",
    "    # Step 6: Evaluation\n",
    "    model.eval()\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            val_predictions.extend(predictions.tolist())\n",
    "            val_labels.extend(labels.tolist())\n",
    "\n",
    "    val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Step 7: Inference\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, attention_mask, _ = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        test_predictions.extend(predictions.tolist())\n",
    "\n",
    "# Create a new DataFrame for inferred labels\n",
    "inferred_labels = pd.DataFrame({'post': test_data['post'], 'inferred_label': test_predictions})\n",
    "inferred_labels.to_csv('inferred_labels.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/brittanyharding/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 1.2376 - Val Accuracy: 0.6500\n",
      "Epoch 2/10 - Train Loss: 1.0975 - Val Accuracy: 0.6500\n",
      "Epoch 3/10 - Train Loss: 1.0579 - Val Accuracy: 0.6500\n",
      "Epoch 4/10 - Train Loss: 1.0043 - Val Accuracy: 0.6500\n",
      "Epoch 5/10 - Train Loss: 0.9952 - Val Accuracy: 0.6500\n",
      "Epoch 6/10 - Train Loss: 0.9804 - Val Accuracy: 0.6500\n",
      "Epoch 7/10 - Train Loss: 0.9579 - Val Accuracy: 0.6500\n",
      "Epoch 8/10 - Train Loss: 0.9338 - Val Accuracy: 0.7000\n",
      "Epoch 9/10 - Train Loss: 0.9199 - Val Accuracy: 0.7000\n",
      "Epoch 10/10 - Train Loss: 0.8986 - Val Accuracy: 0.7000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Data Preparation\n",
    "clean_annotated_posts = pd.read_csv('clean_annotated_posts.csv')\n",
    "\n",
    "# Sample a smaller portion of data\n",
    "sample_size = 100  # Adjust the sample size as desired\n",
    "clean_annotated_posts = clean_annotated_posts.sample(sample_size)\n",
    "\n",
    "# Convert the text labels to numeric labels\n",
    "label_mapping = {\n",
    "    'Crisis': 0,\n",
    "    'Struggling': 1,\n",
    "    'Recovery/Management': 2,\n",
    "    'Support/Advice': 3\n",
    "}\n",
    "\n",
    "clean_annotated_posts['label'] = clean_annotated_posts['classification'].map(label_mapping)\n",
    "\n",
    "# Split the data into train, val, and test sets\n",
    "train_data = clean_annotated_posts.sample(frac=0.6, random_state=42)\n",
    "val_data = clean_annotated_posts.sample(frac=0.2, random_state=42)\n",
    "test_data = clean_annotated_posts.sample(frac=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Fine-tuning BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(clean_annotated_posts['label'].unique()))\n",
    "\n",
    "# Step 3: Dataset Preparation\n",
    "class PostDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.iloc[index]['tokenized_preprocessed_post']\n",
    "        label = self.data.iloc[index]['label']\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoded_input['input_ids'].squeeze()\n",
    "        attention_mask = encoded_input['attention_mask'].squeeze()\n",
    "\n",
    "        # Convert labels to torch.long type\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "\n",
    "max_length = 128\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = PostDataset(train_data, tokenizer, max_length)\n",
    "val_dataset = PostDataset(val_data, tokenizer, max_length)\n",
    "test_dataset = PostDataset(test_data, tokenizer, max_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Step 4: Model Architecture\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Step 5: Training\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = total_loss / total_samples\n",
    "\n",
    "    # Step 6: Evaluation\n",
    "    model.eval()\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            val_predictions.extend(predictions.tolist())\n",
    "            val_labels.extend(labels.tolist())\n",
    "\n",
    "    val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Step 7: Inference\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, attention_mask, _ = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        test_predictions.extend(predictions.tolist())\n",
    "\n",
    "# Create a new DataFrame for inferred labels\n",
    "inferred_labels = pd.DataFrame({'post': test_data['post'], 'inferred_label': test_predictions})\n",
    "inferred_labels.to_csv('inferred_labels.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan]\n"
     ]
    }
   ],
   "source": [
    "print(clean_annotated_posts['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(clean_annotated_posts['classification'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Struggling' 'Support/Advice' 'Crisis' 'Recovery/Management']\n"
     ]
    }
   ],
   "source": [
    "print(clean_annotated_posts['classification'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
